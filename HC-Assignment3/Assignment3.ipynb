{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "favorite-tunnel",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-04-06T12:51:07.072611Z",
     "iopub.status.busy": "2021-04-06T12:51:07.070920Z",
     "iopub.status.idle": "2021-04-06T12:51:20.714935Z",
     "shell.execute_reply": "2021-04-06T12:51:20.716096Z"
    },
    "papermill": {
     "duration": 13.656978,
     "end_time": "2021-04-06T12:51:20.716513",
     "exception": false,
     "start_time": "2021-04-06T12:51:07.059535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from torchsummary import summary\n",
    "    \n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ec988a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘saved_image’: File exists\n"
     ]
    }
   ],
   "source": [
    "! mkdir saved_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "492ccc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "def get_pretrained_unetplusplus(input_channel, output_channel):\n",
    "    model = smp.UnetPlusPlus(\n",
    "        encoder_name=\"resnet34\", # Choose the encoder architecture\n",
    "        encoder_weights=\"imagenet\", # Use pre-trained weights from ImageNet\n",
    "        in_channels=input_channel,\n",
    "        classes=output_channel,\n",
    "        activation=None, # No activation function for the final layer\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd286b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class LitsDataset(Dataset):\n",
    "    def __init__(self, csv_file, base_path, type=\"train\", split_ratio=0.2, transform=None, extra_samples=1000):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.extra = pd.read_csv(csv_file)\n",
    "        self.base_path = base_path\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Filter out liver_mask_empty rows\n",
    "        self.df = self.df[self.df['liver_mask_empty'].isin(['TRUE', True])]\n",
    "        # Add 3000 extra images where liver_mask_empty is False\n",
    "        extra_df = self.extra[self.extra['liver_mask_empty'].isin(['False', False])]\n",
    "        extra_df1 = extra_df.head(extra_samples)\n",
    "        extra_df2 = extra_df.tail(extra_samples)\n",
    "        self.df1 = pd.concat([extra_df1 , self.df], ignore_index=True)\n",
    "        self.df = pd.concat([self.df1, extra_df2], ignore_index=True)\n",
    "\n",
    "        # Train-test split\n",
    "        if type==\"train\":\n",
    "            self.df = self.df.iloc[:int(len(self.df)*(1-split_ratio))]\n",
    "        elif type==\"val\":\n",
    "            self.df = self.df.iloc[int(len(self.df)*(1-split_ratio)):]\n",
    "\n",
    "        self.df['tumor_present'] = self.df['tumor_mask_empty'].apply(lambda x: x not in ['TRUE', True])\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        img_path = os.path.join(self.base_path, row['filepath'])\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"), dtype=np.float32)  # Cast to float32\n",
    "        image = np.transpose(image, (2, 0, 1))\n",
    "\n",
    "        # Load tumor and liver masks\n",
    "        tumor_mask_path = os.path.join(self.base_path, row['tumor_maskpath'])\n",
    "        liver_mask_path = os.path.join(self.base_path, row['liver_maskpath'])\n",
    "        tumor_mask = np.array(Image.open(tumor_mask_path).point(lambda x: x * 255).convert(\"L\"), dtype=np.float32)\n",
    "        liver_mask = np.array(Image.open(liver_mask_path).point(lambda x: x * 255).convert(\"L\"), dtype=np.float32)\n",
    "\n",
    "        # Set mask values\n",
    "        tumor_mask[tumor_mask == 255.0] = 1.0\n",
    "        liver_mask[liver_mask == 255.0] = 1.0\n",
    "\n",
    "        # Create a two-channel mask\n",
    "        mask = np.stack((liver_mask, tumor_mask), axis=0)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            augmentations = self.transform(image=image, mask=mask)\n",
    "            image = augmentations[\"image\"]\n",
    "            mask = augmentations[\"mask\"]\n",
    "\n",
    "        has_tumor = self.df.iloc[index]['tumor_present']\n",
    "\n",
    "        return image, mask, has_tumor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "serious-blond",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-06T12:51:21.655554Z",
     "iopub.status.busy": "2021-04-06T12:51:21.653537Z",
     "iopub.status.idle": "2021-04-06T12:51:21.656309Z",
     "shell.execute_reply": "2021-04-06T12:51:21.656943Z"
    },
    "papermill": {
     "duration": 0.028435,
     "end_time": "2021-04-06T12:51:21.657140",
     "exception": false,
     "start_time": "2021-04-06T12:51:21.628705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint, model):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c2a900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "def get_loaders(\n",
    "    csv_file,\n",
    "    base_path,\n",
    "    batch_size=8,\n",
    "    train_transform=None,\n",
    "    val_transform=None,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "):\n",
    "\n",
    "    train_ds = LitsDataset(\n",
    "        csv_file=csv_file,\n",
    "        base_path=base_path,\n",
    "        type=\"train\",\n",
    "        transform=train_transform,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    val_ds = LitsDataset(\n",
    "        csv_file=csv_file,\n",
    "        base_path=base_path,\n",
    "        type=\"val\",\n",
    "        transform=val_transform,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ffd1fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_val(loader, model, device=\"cuda\"):\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    dice_score_1 = 0\n",
    "    dice_score_2 = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y, has_tumor in loader:  \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            preds_1 = preds[:, 0, :, :]\n",
    "            preds_2 = preds[:, 1, :, :]\n",
    "            y_1 = y[:, 0, :, :]\n",
    "            y_2 = y[:, 1, :, :]\n",
    "            preds_1 = (preds_1 > 0.5).float()\n",
    "            preds_2 = (preds_2 > 0.5).float()\n",
    "            num_correct += ((preds_1 == y_1) & (preds_2 == y_2)).sum()\n",
    "            num_pixels += torch.numel(y_1)\n",
    "            dice_score_1 += (2 * (preds_1 * y_1).sum()) / ((preds_1 + y_1).sum() + 1e-8)\n",
    "            dice_score_2 += (2 * (preds_2 * y_2).sum()) / ((preds_2 + y_2).sum() + 1e-8)\n",
    "\n",
    "    print(f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\")\n",
    "    print(f\"Channel 1 Dice score: {dice_score_1/len(loader)}\")\n",
    "    print(f\"Channel 2 Dice score: {dice_score_2/len(loader)}\")\n",
    "    print(f\"Average Dice score: {(dice_score_1 + dice_score_2) / (2 * len(loader))}\")\n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9484a931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "def check_accuracy(loader, model, device=\"cuda\"):\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    dice_score_1 = 0\n",
    "    dice_score_2 = 0\n",
    "    jaccard_score_1 = 0\n",
    "    jaccard_score_2 = 0\n",
    "    precision_1 = 0\n",
    "    precision_2 = 0\n",
    "    recall_1 = 0\n",
    "    recall_2 = 0\n",
    "    model.eval()\n",
    "    hausdorff_distance_1 = 0\n",
    "    hausdorff_distance_2 = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y , has_tumor in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            preds_1 = preds[:, 0, :, :]\n",
    "            preds_2 = preds[:, 1, :, :]\n",
    "            y_1 = y[:, 0, :, :]\n",
    "            y_2 = y[:, 1, :, :]\n",
    "            preds_1 = (preds_1 > 0.5).float()\n",
    "            preds_2 = (preds_2 > 0.5).float()\n",
    "\n",
    "            intersection_1 = preds_1 * y_1\n",
    "            intersection_2 = preds_2 * y_2\n",
    "\n",
    "            num_correct += (intersection_1.sum() + intersection_2.sum())\n",
    "            num_pixels += (y_1.sum() + y_2.sum())\n",
    "\n",
    "            dice_score_1 += (2 * intersection_1.sum()) / (preds_1.sum() + y_1.sum() + 1e-8)\n",
    "            dice_score_2 += (2 * intersection_2.sum()) / (preds_2.sum() + y_2.sum() + 1e-8)\n",
    "\n",
    "            union_1 = preds_1 + y_1 - intersection_1\n",
    "            union_2 = preds_2 + y_2 - intersection_2\n",
    "            jaccard_score_1 += intersection_1.sum() / (union_1.sum() + 1e-8)\n",
    "            jaccard_score_2 += intersection_2.sum() / (union_2.sum() + 1e-8)\n",
    "\n",
    "            precision_1 += intersection_1.sum() / (preds_1.sum() + 1e-8)\n",
    "            precision_2 += intersection_2.sum() / (preds_2.sum() + 1e-8)\n",
    "            recall_1 += intersection_1.sum() / (y_1.sum() + 1e-8)\n",
    "            recall_2 += intersection_2.sum() / (y_2.sum() + 1e-8)\n",
    "            for i in range(preds_1.shape[0]):\n",
    "                y_np_1 = y_1[i].cpu().numpy()\n",
    "                preds_np_1 = preds_1[i].cpu().numpy()\n",
    "                y_np_2 = y_2[i].cpu().numpy()\n",
    "                preds_np_2 = preds_2[i].cpu().numpy()\n",
    "                \n",
    "                hd1 = directed_hausdorff(y_np_1, preds_np_1)[0]\n",
    "                hd2 = directed_hausdorff(y_np_2, preds_np_2)[0]\n",
    "                \n",
    "                hausdorff_distance_1 += hd1\n",
    "                hausdorff_distance_2 += hd2\n",
    "\n",
    "    n = len(loader)\n",
    "    print(f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\")\n",
    "    print(f\"Channel 1 Dice score: {dice_score_1/n}\")\n",
    "    print(f\"Channel 2 Dice score: {dice_score_2/n}\")\n",
    "    print(f\"Average Dice score: {(dice_score_1 + dice_score_2) / (2 * n)}\")\n",
    "    print(f\"Channel 1 Jaccard score: {jaccard_score_1/n}\")\n",
    "    print(f\"Channel 2 Jaccard score: {jaccard_score_2/n}\")\n",
    "    print(f\"Average Jaccard score: {(jaccard_score_1 + jaccard_score_2) / (2 * n)}\")\n",
    "    print(f\"Channel 1 Precision: {precision_1/n}\")\n",
    "    print(f\"Channel 2 Precision: {precision_2/n}\")\n",
    "    print(f\"Average Precision: {(precision_1 + precision_2) / (2 * n)}\")\n",
    "    print(f\"Channel 1 Recall: {recall_1/n}\")\n",
    "    print(f\"Channel 2 Recall: {recall_2/n}\")\n",
    "    print(f\"Average Recall: {(recall_1 + recall_2) / (2 * n)}\")\n",
    "    print(f\"Channel 1 F1-score: {2 * (precision_1 * recall_1) / (precision_1 + recall_1 + 1e-8)/n}\")\n",
    "    print(f\"Channel 2 F1-score: {2 * (precision_2 * recall_2) / (precision_2 + recall_2 + 1e-8)/n}\")\n",
    "    print(f\"Average F1-score: {(2 * (precision_1 * recall_1) + 2 * (precision_2 * recall_2)) / (precision_1 + recall_1 + precision_2 + recall_2 + 1e-8)/n}\")\n",
    "    n = len(loader) * x.shape[0]\n",
    "    print(f\"Channel 1 Hausdorff distance: {hausdorff_distance_1/n}\")\n",
    "    print(f\"Channel 2 Hausdorff distance: {hausdorff_distance_2/n}\")\n",
    "    print(f\"Average Hausdorff distance: {(hausdorff_distance_1 + hausdorff_distance_2) / (2 * n)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f5780f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions_as_imgs(loader, model, folder=\"saved_image/\", device=\"cuda\"):\n",
    "    model.eval()\n",
    "    for idx, (x, y, has_tumor) in enumerate(loader):\n",
    "        x = x.to(device=device)\n",
    "        with torch.no_grad():\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            preds = (preds > 0.5).float()\n",
    "\n",
    "        # Save liver masks and predictions\n",
    "        torchvision.utils.save_image(y[:, 0:1, :, :], f\"{folder}/mask_liver_{idx}.png\")\n",
    "        torchvision.utils.save_image(preds[:, 0:1, :, :], f\"{folder}/pred_liver_{idx}.png\")\n",
    "\n",
    "        # Save tumor masks and predictions only if has_tumor is True\n",
    "        if has_tumor.any():\n",
    "            torchvision.utils.save_image(y[:, 1:2, :, :], f\"{folder}/mask_tumor_{idx}.png\")\n",
    "            torchvision.utils.save_image(preds[:, 1:2, :, :], f\"{folder}/pred_tumor_{idx}.png\")\n",
    "\n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "latest-devil",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-06T12:51:21.683026Z",
     "iopub.status.busy": "2021-04-06T12:51:21.681160Z",
     "iopub.status.idle": "2021-04-06T12:51:21.683597Z",
     "shell.execute_reply": "2021-04-06T12:51:21.684017Z"
    },
    "papermill": {
     "duration": 0.01745,
     "end_time": "2021-04-06T12:51:21.684168",
     "exception": false,
     "start_time": "2021-04-06T12:51:21.666718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "lr = 1e-6\n",
    "dev = \"cuda\"\n",
    "batch_size = 16\n",
    "epochs = 10\n",
    "workers= 8\n",
    "img_h = 256\n",
    "img_w = 256\n",
    "pin_mem= True\n",
    "load_model = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "defensive-heading",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-06T12:51:21.712883Z",
     "iopub.status.busy": "2021-04-06T12:51:21.710884Z",
     "iopub.status.idle": "2021-04-06T12:51:21.713492Z",
     "shell.execute_reply": "2021-04-06T12:51:21.713913Z"
    },
    "papermill": {
     "duration": 0.019925,
     "end_time": "2021-04-06T12:51:21.714048",
     "exception": false,
     "start_time": "2021-04-06T12:51:21.694123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_fn(loader, model, optimizer, loss_fn, scaler, device):\n",
    "    loop = tqdm(loader)\n",
    "\n",
    "    for batch_idx, (data, targets, has_tumor) in enumerate(loop):\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.float().to(device=device)\n",
    "\n",
    "        # forward\n",
    "        with torch.cuda.amp.autocast():\n",
    "            predictions = model(data)\n",
    "            \n",
    "            # Calculate loss for liver masks (always)\n",
    "            liver_targets = targets[:, 0, :, :]\n",
    "            liver_outputs = predictions[:, 0, :, :]\n",
    "            liver_loss = loss_fn(liver_outputs, liver_targets)\n",
    "\n",
    "            # Calculate loss for tumor masks (only when has_tumor is True)\n",
    "            if has_tumor.any():\n",
    "                tumor_targets = targets[:, 1, :, :]\n",
    "                tumor_outputs = predictions[:, 1, :, :]\n",
    "                tumor_loss = loss_fn(tumor_outputs, tumor_targets)\n",
    "                loss = liver_loss + tumor_loss\n",
    "            else:\n",
    "                loss = liver_loss\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # update tqdm loop\n",
    "        loop.set_postfix(loss=loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "479a51c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(loader, model, device, criterion):\n",
    "    model.eval()\n",
    "    liver_total_loss = 0.0\n",
    "    tumor_total_loss = 0.0\n",
    "    num_samples = 0\n",
    "    num_samples_with_tumor = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            inputs, targets, has_tumor = data\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Calculate loss for liver masks (always)\n",
    "            liver_targets = targets[:, 0, :, :]\n",
    "            liver_outputs = outputs[:, 0, :, :]\n",
    "            liver_loss = criterion(liver_outputs, liver_targets)\n",
    "            liver_total_loss += liver_loss.item() * inputs.size(0)\n",
    "            \n",
    "            # Calculate loss for tumor masks (only when any item in the batch has a tumor)\n",
    "            if has_tumor.any():\n",
    "                tumor_targets = targets[:, 1, :, :]\n",
    "                tumor_outputs = outputs[:, 1, :, :]\n",
    "                tumor_loss = criterion(tumor_outputs, tumor_targets)\n",
    "                tumor_total_loss += tumor_loss.item() * inputs.size(0)\n",
    "                num_samples_with_tumor += inputs.size(0)\n",
    "            num_samples += inputs.size(0)\n",
    "        \n",
    "        avg_liver_loss = liver_total_loss / num_samples\n",
    "        avg_tumor_loss = tumor_total_loss / num_samples_with_tumor if num_samples_with_tumor > 0 else 0\n",
    "    \n",
    "    return avg_liver_loss, avg_tumor_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61155873",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.loss import _Loss\n",
    "import torch.nn as nn\n",
    "\n",
    "class WeightedDiceLoss(nn.Module):\n",
    "    def __init__(self, weights, mode='multilabel', eps=1e-7):\n",
    "        super(WeightedDiceLoss, self).__init__()\n",
    "        self.weights = weights\n",
    "        self.mode = mode\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        assert input.size() == target.size(), \"Input and target must have the same size\"\n",
    "        input = torch.sigmoid(input)\n",
    "        \n",
    "        if self.mode == 'multilabel':\n",
    "            weight_map = torch.zeros_like(target)\n",
    "            for i, w in enumerate(self.weights):\n",
    "                weight_map[target == i] = w\n",
    "            intersection = torch.sum(weight_map * input * target)\n",
    "            union = torch.sum(weight_map * (input + target))\n",
    "        else:\n",
    "            intersection = torch.sum(input * target)\n",
    "            union = torch.sum(input + target)\n",
    "\n",
    "        dice = (2 * intersection + self.eps) / (union + self.eps)\n",
    "\n",
    "        return 1 - dice\n",
    "weights = torch.tensor([0.5, 5]).to(dev)  # Give more weight to white pixels (class 1) and less to black pixels (class 0)\n",
    "loss_fn = WeightedDiceLoss(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6648df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "def get_transforms(resize_height, resize_width):\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(resize_height, resize_width),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        #A.Normalize(),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(resize_height, resize_width),\n",
    "        #A.Normalize(),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    return train_transform, val_transform\n",
    "train_transform, val_transform = get_transforms(resize_height=256, resize_width=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "religious-cause",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-06T12:51:21.748520Z",
     "iopub.status.busy": "2021-04-06T12:51:21.743088Z",
     "iopub.status.idle": "2021-04-06T13:23:25.415917Z",
     "shell.execute_reply": "2021-04-06T13:23:25.415330Z"
    },
    "papermill": {
     "duration": 1923.690808,
     "end_time": "2021-04-06T13:23:25.416069",
     "exception": false,
     "start_time": "2021-04-06T12:51:21.725261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/177 [00:00<?, ?it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f7a886a9c60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahmed.sharshar/.conda/envs/new/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/ahmed.sharshar/.conda/envs/new/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1412, in _shutdown_workers\n",
      "    self._pin_memory_thread.join()\n",
      "  File \"/home/ahmed.sharshar/.conda/envs/new/lib/python3.10/threading.py\", line 1096, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/ahmed.sharshar/.conda/envs/new/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n",
      "    if lock.acquire(block, timeout):\n",
      "KeyboardInterrupt: \n",
      "  0%|          | 0/177 [03:35<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 7, 7], expected input[64, 256, 256, 256] to have 3 channels, but got 256 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m     50\u001b[0m     \u001b[39mprint\u001b[39m(epoch)\n\u001b[0;32m---> 51\u001b[0m     train_fn(train_loader, model, optimizer, loss_fn, scaler, device\u001b[39m=\u001b[39;49mdev)\n\u001b[1;32m     52\u001b[0m     val_loss \u001b[39m=\u001b[39m eval_fn(val_loader, model, device\u001b[39m=\u001b[39mdev, criterion\u001b[39m=\u001b[39mloss_fn)\n\u001b[1;32m     53\u001b[0m     avg_liver_loss, avg_tumor_loss \u001b[39m=\u001b[39m eval_fn(val_loader, model, device\u001b[39m=\u001b[39mdev, criterion\u001b[39m=\u001b[39mloss_fn)\n",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m, in \u001b[0;36mtrain_fn\u001b[0;34m(loader, model, optimizer, loss_fn, scaler, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39m# forward\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast():\n\u001b[0;32m---> 12\u001b[0m     predictions \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m     14\u001b[0m     \u001b[39m# Calculate loss for liver masks (always)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     liver_targets \u001b[39m=\u001b[39m targets[:, \u001b[39m0\u001b[39m, :, :]\n",
      "File \u001b[0;32m~/.conda/envs/new/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/new/lib/python3.10/site-packages/segmentation_models_pytorch/base/model.py:29\u001b[0m, in \u001b[0;36mSegmentationModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_input_shape(x)\n\u001b[0;32m---> 29\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[1;32m     30\u001b[0m decoder_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(\u001b[39m*\u001b[39mfeatures)\n\u001b[1;32m     32\u001b[0m masks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msegmentation_head(decoder_output)\n",
      "File \u001b[0;32m~/.conda/envs/new/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/new/lib/python3.10/site-packages/segmentation_models_pytorch/encoders/resnet.py:62\u001b[0m, in \u001b[0;36mResNetEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m features \u001b[39m=\u001b[39m []\n\u001b[1;32m     61\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_depth \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m---> 62\u001b[0m     x \u001b[39m=\u001b[39m stages[i](x)\n\u001b[1;32m     63\u001b[0m     features\u001b[39m.\u001b[39mappend(x)\n\u001b[1;32m     65\u001b[0m \u001b[39mreturn\u001b[39;00m features\n",
      "File \u001b[0;32m~/.conda/envs/new/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/new/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/new/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/new/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.conda/envs/new/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 7, 7], expected input[64, 256, 256, 256] to have 3 channels, but got 256 channels instead"
     ]
    }
   ],
   "source": [
    "\n",
    "input_channel = 3\n",
    "output_channel = 2  # Updated from 1 to 2\n",
    "model = get_pretrained_unetplusplus(input_channel, output_channel).to(dev)\n",
    "\n",
    "from segmentation_models_pytorch.losses import DiceLoss\n",
    "#loss_fn = DiceLoss(mode='multilabel')\n",
    "\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-4 ,  weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "train_loader, val_loader = get_loaders(\n",
    "    csv_file=\"/apps/local/shared/HC701/assessment/assignment_3/data/hc701_lits_train.csv\",\n",
    "    base_path=\"/apps/local/shared/HC701/assessment/assignment_3/data/\",\n",
    "    batch_size=64,\n",
    "    train_transform=None,\n",
    "    val_transform=None,\n",
    ")\n",
    "\n",
    "#check_accuracy(val_loader, model, device=dev)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# test_accuracy(test_loader, model, device=dev)\n",
    "# check_accuracy(val_loader, model, device=dev)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(epoch)\n",
    "    train_fn(train_loader, model, optimizer, loss_fn, scaler, device=dev)\n",
    "    val_loss = eval_fn(val_loader, model, device=dev, criterion=loss_fn)\n",
    "    avg_liver_loss, avg_tumor_loss = eval_fn(val_loader, model, device=dev, criterion=loss_fn)\n",
    "    avg_loss = avg_liver_loss*0.1 + avg_tumor_loss*0.9\n",
    "    scheduler.step(avg_loss)\n",
    "\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\":optimizer.state_dict(),\n",
    "    }\n",
    "    # check_accuracy(test_loader, model, device=dev)\n",
    "    # check_accuracy(val_loader, model, device=dev)\n",
    "    check_val(val_loader, model, device=dev)\n",
    "\n",
    "    save_predictions_as_imgs(val_loader, model, folder=\"saved_image/\", device=dev)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40b922c2",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba73ed58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitsTestDataset(Dataset):\n",
    "    def __init__(self, csv_file, base_path, transform=None):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.base_path = base_path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        img_path = os.path.join(self.base_path, row['filepath'])\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"), dtype=np.float32)  # Cast to float32\n",
    "        image = np.transpose(image, (2, 0, 1))\n",
    "\n",
    "        # Load tumor and liver masks\n",
    "        tumor_mask_path = os.path.join(self.base_path, row['tumor_maskpath'])\n",
    "        liver_mask_path = os.path.join(self.base_path, row['liver_maskpath'])\n",
    "        tumor_mask = np.array(Image.open(tumor_mask_path).point(lambda x: x * 255).convert(\"L\"), dtype=np.float32)\n",
    "        liver_mask = np.array(Image.open(liver_mask_path).point(lambda x: x * 255).convert(\"L\"), dtype=np.float32)\n",
    "\n",
    "        # Set mask values\n",
    "        tumor_mask[tumor_mask == 255.0] = 1.0\n",
    "        liver_mask[liver_mask == 255.0] = 1.0\n",
    "\n",
    "        # Create a two-channel mask\n",
    "        mask = np.stack((liver_mask, tumor_mask), axis=0)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            augmentations = self.transform(image=image, mask=mask)\n",
    "            image = augmentations[\"image\"]\n",
    "            mask = augmentations[\"mask\"]\n",
    "\n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8d03c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loaders(\n",
    "    csv_file,\n",
    "    base_path,\n",
    "    batch_size=4,\n",
    "    test_transform=None,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    "):\n",
    "    test_ds = LitsTestDataset(\n",
    "    csv_file=csv_file,\n",
    "    base_path=base_path,\n",
    "    transform=test_transform,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ca401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = test_loaders(\n",
    "    csv_file=\"/apps/local/shared/HC701/assessment/assignment_3/data/hc701_lits_test.csv\",\n",
    "    base_path=\"/apps/local/shared/HC701/assessment/assignment_3/data/\",\n",
    "    batch_size=16,\n",
    "    test_transform=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a93f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "def test_accuracy(loader, model, device=\"cuda\"):\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    dice_score_1 = 0\n",
    "    dice_score_2 = 0\n",
    "    jaccard_score_1 = 0\n",
    "    jaccard_score_2 = 0\n",
    "    precision_1 = 0\n",
    "    precision_2 = 0\n",
    "    recall_1 = 0\n",
    "    recall_2 = 0\n",
    "    model.eval()\n",
    "    hausdorff_distance_1 = 0\n",
    "    hausdorff_distance_2 = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            preds_1 = preds[:, 0, :, :]\n",
    "            preds_2 = preds[:, 1, :, :]\n",
    "            y_1 = y[:, 0, :, :]\n",
    "            y_2 = y[:, 1, :, :]\n",
    "            preds_1 = (preds_1 > 0.5).float()\n",
    "            preds_2 = (preds_2 > 0.5).float()\n",
    "\n",
    "            intersection_1 = preds_1 * y_1\n",
    "            intersection_2 = preds_2 * y_2\n",
    "\n",
    "            num_correct += (intersection_1.sum() + intersection_2.sum())\n",
    "            num_pixels += (y_1.sum() + y_2.sum())\n",
    "\n",
    "            dice_score_1 += (2 * intersection_1.sum()) / (preds_1.sum() + y_1.sum() + 1e-8)\n",
    "            dice_score_2 += (2 * intersection_2.sum()) / (preds_2.sum() + y_2.sum() + 1e-8)\n",
    "\n",
    "            union_1 = preds_1 + y_1 - intersection_1\n",
    "            union_2 = preds_2 + y_2 - intersection_2\n",
    "            jaccard_score_1 += intersection_1.sum() / (union_1.sum() + 1e-8)\n",
    "            jaccard_score_2 += intersection_2.sum() / (union_2.sum() + 1e-8)\n",
    "\n",
    "            precision_1 += intersection_1.sum() / (preds_1.sum() + 1e-8)\n",
    "            precision_2 += intersection_2.sum() / (preds_2.sum() + 1e-8)\n",
    "            recall_1 += intersection_1.sum() / (y_1.sum() + 1e-8)\n",
    "            recall_2 += intersection_2.sum() / (y_2.sum() + 1e-8)\n",
    "            for i in range(preds_1.shape[0]):\n",
    "                y_np_1 = y_1[i].cpu().numpy()\n",
    "                preds_np_1 = preds_1[i].cpu().numpy()\n",
    "                y_np_2 = y_2[i].cpu().numpy()\n",
    "                preds_np_2 = preds_2[i].cpu().numpy()\n",
    "                \n",
    "                hd1 = directed_hausdorff(y_np_1, preds_np_1)[0]\n",
    "                hd2 = directed_hausdorff(y_np_2, preds_np_2)[0]\n",
    "                \n",
    "                hausdorff_distance_1 += hd1\n",
    "                hausdorff_distance_2 += hd2\n",
    "\n",
    "    n = len(loader)\n",
    "    print(f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\")\n",
    "    print(f\"Channel 1 Dice score: {dice_score_1/n}\")\n",
    "    print(f\"Channel 2 Dice score: {dice_score_2/n}\")\n",
    "    print(f\"Average Dice score: {(dice_score_1 + dice_score_2) / (2 * n)}\")\n",
    "    print(f\"Channel 1 Jaccard score: {jaccard_score_1/n}\")\n",
    "    print(f\"Channel 2 Jaccard score: {jaccard_score_2/n}\")\n",
    "    print(f\"Average Jaccard score: {(jaccard_score_1 + jaccard_score_2) / (2 * n)}\")\n",
    "    print(f\"Channel 1 Precision: {precision_1/n}\")\n",
    "    print(f\"Channel 2 Precision: {precision_2/n}\")\n",
    "    print(f\"Average Precision: {(precision_1 + precision_2) / (2 * n)}\")\n",
    "    print(f\"Channel 1 Recall: {recall_1/n}\")\n",
    "    print(f\"Channel 2 Recall: {recall_2/n}\")\n",
    "    print(f\"Average Recall: {(recall_1 + recall_2) / (2 * n)}\")\n",
    "    print(f\"Channel 1 F1-score: {2 * (precision_1 * recall_1) / (precision_1 + recall_1 + 1e-8)/n}\")\n",
    "    print(f\"Channel 2 F1-score: {2 * (precision_2 * recall_2) / (precision_2 + recall_2 + 1e-8)/n}\")\n",
    "    print(f\"Average F1-score: {(2 * (precision_1 * recall_1) + 2 * (precision_2 * recall_2)) / (precision_1 + recall_1 + precision_2 + recall_2 + 1e-8)/n}\")\n",
    "    n = len(loader) * x.shape[0]\n",
    "    print(f\"Channel 1 Hausdorff distance: {hausdorff_distance_1/n}\")\n",
    "    print(f\"Channel 2 Hausdorff distance: {hausdorff_distance_2/n}\")\n",
    "    print(f\"Average Hausdorff distance: {(hausdorff_distance_1 + hausdorff_distance_2) / (2 * n)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941d1529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 6738262.0/7501262.0 with acc 89.83\n",
      "Channel 1 Dice score: 0.5945385694503784\n",
      "Channel 2 Dice score: 0.1304144561290741\n",
      "Average Dice score: 0.36247649788856506\n",
      "Channel 1 Jaccard score: 0.5519060492515564\n",
      "Channel 2 Jaccard score: 0.09957405924797058\n",
      "Average Jaccard score: 0.3257400691509247\n",
      "Channel 1 Precision: 0.5725361108779907\n",
      "Channel 2 Precision: 0.2362295538187027\n",
      "Average Precision: 0.4043827950954437\n",
      "Channel 1 Recall: 0.6336069703102112\n",
      "Channel 2 Recall: 0.1109042838215828\n",
      "Average Recall: 0.3722556233406067\n",
      "Channel 1 F1-score: 0.6015254259109497\n",
      "Channel 2 F1-score: 0.15094390511512756\n",
      "Average F1-score: 0.5008273124694824\n",
      "Channel 1 Hausdorff distance: 9.626958103826295\n",
      "Channel 2 Hausdorff distance: 7.44515377091632\n",
      "Average Hausdorff distance: 8.536055937371307\n"
     ]
    }
   ],
   "source": [
    "test_accuracy(test_loader, model, device=dev)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f67998d28f6fc57a183a22e7a0309b9a80aa8713c373b12cd502dd672f9616c8"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1947.884009,
   "end_time": "2021-04-06T13:23:28.655243",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-04-06T12:51:00.771234",
   "version": "2.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
